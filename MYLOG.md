# 依赖于 scrapy_redis 框架
# 因为需要修改到连接 redis 和处理新的 redis 数据存储的方式
# 并且又不能单纯的依赖于 scrapy_redis
# 所以直接拿 scrapy_redis 进行直接改造，主要是在 redis 的调度以及任务信息的存储上面进行一定程度的魔改。
# 考虑到多任务实现的框架的处理，所以就需要在一定情况下面进行更加细化的 id存储的调配处理
# 其中一些核心组件使用到了 scrapy 本身就有的组件，所以这部分也需要进行以插件形式的魔改
# 这样处理会更集成一点，并且在一定程度上依赖会小一些，不需要对 scrapy_redis 进行二次下载

# 1/ 考虑 spiderid   # 这个是考虑对 spiderid 执行状态的监控
# 2/ 考虑 taskid     # 这个是考虑对 taskid   任务主要的统计

# 后续的处理再看，先考虑实现功能

#20190505
# 目前的问题主要是在初始化上面，这里的初始化主要指的是 redis 数据结构设计
# 一方面要兼具 spiderid 和 taskid，另外还要考虑初始化的时机，确实有点麻烦的。

#20190510
# 继续补充注释的内容，后续可能要考虑怎么处理后续的结构上的问题。
# 处理多任务时候怎么让工具更加适配
# 下次的更新应该会再 scrapy_redis_mod.spiders 这里，主要是实现对任务的调度和处理上。
# 后续还可能需要突然死机后的重启恢复任务解决方式。很重要。

#20190511
# 尝试解决脚本传递的技术难点,力求无需特殊脚本处理的脚本传递方法.
# 尽量在不改变原脚本的基础上实现功能，都是为了方便而已
# 钩子还是稍微不够长，response 没有钩到，导致请求后回到 parse 函数内的 response 都没有我设置的传递参数
# 所以需要再测试几次在什么地方挂钩才能钩到这一个参数

#20190512
# 发现在中间件里面进行挂钩非常的合适，解决了之前各种各样的问题
# 现在的问题主要出在于怎么将脚本的执行处理干净，经量将脚本传递变得更加方便一些。
# 为了尽量不破坏源代码，所以就要尽可能的让脚本常驻环境当中，所以，目前将选择使用文件存储的方式来生成文件
# 用传递脚本的 hash 作为文件名字存储，这样也能对重复的脚本进行去重处理
#     最后尽然以一种不可思的方式实现了主体的功能。
#     后续可能需要考虑一些爬虫的配置的传递方面的事情。
# 目前已经实现了多任务的 scrapy 分布式，不过在于scrapy的配置上面可能还不够好
# 所以后续需要多看看有哪些配置能够通过 custom_settings 进行传递的
# 这样会比较方便于后面的开发。
# 另一方面就是在日志显示上面的开发可能还需要多花点时间进行处理。